{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume your data is in a pandas DataFrame called 'data'\n",
    "x = df.drop('y', axis=1) # input features\n",
    "y = df['y'] # target variable\n",
    "\n",
    "# split the data into training and validation sets with 80% for training\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.fillna(0)\n",
    "x_val = x_val.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the columns of x_train\n",
    "scaler = MinMaxScaler()\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "y_train_normalized = y_train - 1\n",
    "# y_train_normalized = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Convert x_train and y_train to PyTorch tensors\n",
    "x_train_tensor = torch.Tensor(x_train_normalized)\n",
    "y_train_tensor = torch.Tensor(y_train_normalized.values)\n",
    "\n",
    "# Create a TensorDataset for x_train and y_train\n",
    "# train_dataset = TensorDataset(x_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(37, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_normalized), y=y_train_normalized)\n",
    "weights = compute_class_weight(class_weight={1:5, 0:1}, classes=np.unique(y_train_normalized), y=y_train_normalized)\n",
    "sample_weights = torch.tensor([weights[cls] for cls in y_train_normalized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss(weight=sample_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 0|2500 the loss is: 0.8602239489555359\n",
      "In 100|2500 the loss is: 0.16723892092704773\n",
      "In 200|2500 the loss is: 0.07789908349514008\n",
      "In 300|2500 the loss is: 0.042769718915224075\n",
      "In 400|2500 the loss is: 0.041186295449733734\n",
      "In 500|2500 the loss is: 0.03392040356993675\n",
      "In 600|2500 the loss is: 0.02827925607562065\n",
      "In 700|2500 the loss is: 0.022245338186621666\n",
      "In 800|2500 the loss is: 0.015899116173386574\n",
      "In 900|2500 the loss is: 0.008162301033735275\n",
      "In 1000|2500 the loss is: 0.004333669785410166\n",
      "In 1100|2500 the loss is: 0.0026602735742926598\n",
      "In 1200|2500 the loss is: 0.0017937816446647048\n",
      "In 1300|2500 the loss is: 0.0012909072684124112\n",
      "In 1400|2500 the loss is: 0.0009733779588714242\n",
      "In 1500|2500 the loss is: 0.0007612625486217439\n",
      "In 1600|2500 the loss is: 0.0006097671575844288\n",
      "In 1700|2500 the loss is: 0.0004996348870918155\n",
      "In 1800|2500 the loss is: 0.00041454468737356365\n",
      "In 1900|2500 the loss is: 0.0003496503923088312\n",
      "In 2000|2500 the loss is: 0.0002982827718369663\n",
      "In 2100|2500 the loss is: 0.0002569988137111068\n",
      "In 2200|2500 the loss is: 0.00022324512246996164\n",
      "In 2300|2500 the loss is: 0.00019534530292730778\n",
      "In 2400|2500 the loss is: 0.00017196679254993796\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 2500\n",
    "for epoch in range(num_epochs):\n",
    "    # outputs = torch.round(model(x_train_tensor))\n",
    "    outputs = model(x_train_tensor)\n",
    "    targets = y_train_tensor.view(-1)  # Convert targets to long data type and flatten\n",
    "    loss = criterion(outputs.view(-1), targets)  # Reshape outputs to match target size\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print(f'In {epoch}|{num_epochs} the loss is: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the columns of x_val\n",
    "x_val_normalized = scaler.fit_transform(x_val)\n",
    "y_val_normalized = y_val - 1\n",
    "\n",
    "# Convert x_val and y_val to PyTorch tensors\n",
    "x_val_tensor = torch.Tensor(x_val_normalized)\n",
    "y_val_tensor = torch.Tensor(y_val_normalized.values)\n",
    "\n",
    "# Create a TensorDataset for x_val and y_val\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create a DataLoader for the validation dataset\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6784\n",
      "Correct Predictions: 192/200\n",
      "Validation Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "val_criterion = nn.BCELoss()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(outputs).squeeze().long()  # Squeeze and convert to long\n",
    "        total_val_loss += val_criterion(outputs.view(-1), targets).item()\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "val_loss = total_val_loss / len(val_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Correct Predictions: {correct}/{total}')\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958318701782389"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.array(y_val_normalized).reshape(-1,1), torch.round(model(x_val_tensor)).detach().numpy(), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7671, 0.3034, 0.3297,  ..., 0.4099, 0.0000, 0.0000],\n",
       "        [0.3591, 0.6307, 0.6129,  ..., 0.1156, 0.0000, 0.0000],\n",
       "        [0.4859, 0.2978, 0.2459,  ..., 0.0663, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.9036, 0.5929, 0.3801,  ..., 0.1531, 0.0000, 0.0000],\n",
       "        [0.3665, 0.5872, 0.4438,  ..., 0.0816, 0.0000, 0.0000],\n",
       "        [0.8895, 0.3518, 0.2517,  ..., 0.0221, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.read_csv('test.csv')\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "x_test_normalized = scaler.fit_transform(x_test)\n",
    "\n",
    "# Convert x_train and y_train to PyTorch tensors\n",
    "x_test_tensor = torch.Tensor(x_test_normalized)\n",
    "x_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = torch.round(model(x_test_tensor)).detach().numpy()\n",
    "test_out = test_out + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test_nn2.txt', test_out, delimiter=',')   # test_out is an array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
